"""
Document Loader –¥–ª—è RAG —Å–∏—Å—Ç–µ–º–∞
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ knowledge base –Ω–∞ —á–∞–Ω–∫–∏
"""

import os
import logging
from typing import List, Optional
from pathlib import Path

try:
    from langchain.document_loaders import TextLoader, DirectoryLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError:
    raise ImportError("LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install langchain")

logger = logging.getLogger(__name__)


class DocumentLoader:
    """
    –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è knowledge base
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        docs_path (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (default: "knowledge_base")
        chunk_size (int): –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ –∫—É—Å–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (default: 1000)
        chunk_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É –∫—É—Å–∫–∞–º–∏ (default: 200)
    """
    
    def __init__(
        self, 
        docs_path: str = "knowledge_base",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        self.docs_path = docs_path
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        
        logger.info(
            f"DocumentLoader –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: "
            f"path={docs_path}, chunk_size={chunk_size}, overlap={chunk_overlap}"
        )
    
    def load_documents(self) -> List:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∑–Ω–∞–Ω–∏–µ–≤–æ–π –±–∞–∑—ã
        
        Returns:
            List: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–±–∏—Ç—ã—Ö –Ω–∞ —á–∞–Ω–∫–∏
        """
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ '{self.docs_path}'...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–∫–∏
        if not os.path.exists(self.docs_path):
            logger.warning(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.docs_path}")
            return []
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö .txt —Ñ–∞–π–ª–æ–≤ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
            loader = DirectoryLoader(
                self.docs_path,
                glob="**/*.txt",
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8'},
                recursive=True
            )
            
            documents = loader.load()
            logger.info(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            if not documents:
                logger.warning("–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ!")
                return []
            
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
            logger.info(f"–†–∞–∑–±–∏–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä={self.chunk_size})...")
            chunks = self.text_splitter.split_documents(documents)
            logger.info(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            chunks = self.add_metadata(chunks)
            
            return chunks
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}")
            raise
    
    def add_metadata(self, chunks: List) -> List:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫ —á–∞–Ω–∫–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            chunks (List): –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        
        Returns:
            List: –ß–∞–Ω–∫–∏ —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        for i, chunk in enumerate(chunks):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            source = chunk.metadata.get('source', '')
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if 'decision_85' in source.lower():
                chunk.metadata['type'] = 'regulation_russia'
                chunk.metadata['authority'] = 'EEC'
            elif 'ema' in source:
                chunk.metadata['type'] = 'regulation_international'
                chunk.metadata['authority'] = 'EMA'
            elif 'fda' in source:
                chunk.metadata['type'] = 'regulation_international'
                chunk.metadata['authority'] = 'FDA'
            elif 'protocol' in source:
                chunk.metadata['type'] = 'example_protocol'
            else:
                chunk.metadata['type'] = 'general'
        
        return chunks
```

---

## üî¢ **–®–∞–≥ 2: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (Embeddings)**

### **2.1 –ß—Ç–æ —Ç–∞–∫–æ–µ Embeddings**
```
–¢–µ–∫—Å—Ç: "–ü—Ä–∏ CV > 50% —Ç—Ä–µ–±—É–µ—Ç—Å—è 4-way replicate –¥–∏–∑–∞–π–Ω"
       ‚Üì (Embedding model)
Vector: [0.23, -0.45, 0.89, ..., 0.12]  # 384 —á–∏—Å–ª–∞
